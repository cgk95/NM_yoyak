{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_src.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9o6eHCBvL5Cf"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNbSptZgrlhcgFWJ7WaZ0MG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUSCyNMsDBVy",
        "outputId": "7c135b3d-0a7e-4f98-a843-ffc6530dfcdc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxRBhmiJDLiH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmyfIMoiDYXx",
        "outputId": "ccb4b6ac-cd1c-4a60-9b5d-4c78b6a0bc23"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 21 04:00:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvxOCaoPNtfK",
        "outputId": "1f7d6d16-a0c4-453b-eb57-9427942949e6"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRwryq9NNx84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKyzcInVDZlS"
      },
      "source": [
        "# 데이터 다운로드 및 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urpFNsaKDyxA",
        "outputId": "2f209ddc-b226-49be-88aa-8857f7ffe2dc"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/datacap/final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/datacap/final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBtAKJAXDZLh"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datacap/final/Cleand_newsdata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "mjv_dfkVD4mc",
        "outputId": "c1311fc1-cb9d-44a9-e1bb-dc53ae07d0c9"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>exbank official booked for cheating bank of  c...</td>\n",
              "      <td>the cbi on saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>suprtheme court to go paperless in  month  cji</td>\n",
              "      <td>chief justice js khehar ha said the suprtheme ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>at least  killed   injured in blast in sylhet ...</td>\n",
              "      <td>at least three people were killed  including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>why ha reliance been barred from trading in fu...</td>\n",
              "      <td>mukesh ambaniled reliance industries  ril  wa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>was stopped from entering my own studio at tim...</td>\n",
              "      <td>tv news anchor arnab goswami ha said he wa tol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55099</th>\n",
              "      <td>55099</td>\n",
              "      <td>sensex loses  point to hit week low</td>\n",
              "      <td>tracking weak cue from the asian ets  the benc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55100</th>\n",
              "      <td>55100</td>\n",
              "      <td>china to inject   bn into the money ets</td>\n",
              "      <td>amid growing concern about china     s economi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55101</th>\n",
              "      <td>55101</td>\n",
              "      <td>ghulam ali set to make acting debut in bollywood</td>\n",
              "      <td>pakistani ghazal singer ghulam ali will soon m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55102</th>\n",
              "      <td>55102</td>\n",
              "      <td>is acknowledges death of jihadi john  report</td>\n",
              "      <td>the islamic state  is  ha acknowledged the dea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55103</th>\n",
              "      <td>55103</td>\n",
              "      <td>cairn to seek   mn from india in damage</td>\n",
              "      <td>ukbased oil firm cairn energy on tuesday said ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55104 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                              Short\n",
              "0               0  ...  the cbi on saturday booked four former officia...\n",
              "1               1  ...  chief justice js khehar ha said the suprtheme ...\n",
              "2               2  ...  at least three people were killed  including a...\n",
              "3               3  ...  mukesh ambaniled reliance industries  ril  wa ...\n",
              "4               4  ...  tv news anchor arnab goswami ha said he wa tol...\n",
              "...           ...  ...                                                ...\n",
              "55099       55099  ...  tracking weak cue from the asian ets  the benc...\n",
              "55100       55100  ...  amid growing concern about china     s economi...\n",
              "55101       55101  ...  pakistani ghazal singer ghulam ali will soon m...\n",
              "55102       55102  ...  the islamic state  is  ha acknowledged the dea...\n",
              "55103       55103  ...  ukbased oil firm cairn energy on tuesday said ...\n",
              "\n",
              "[55104 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "Uvl5YhvrjPTN",
        "outputId": "ac468821-3964-44b2-c5e0-da82a2242457"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "# 데이터 섞기 \n",
        "data = shuffle(data)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19972</th>\n",
              "      <td>19972</td>\n",
              "      <td>kohli average  in  inning at vizag</td>\n",
              "      <td>virat kohli average  in five inning at the apc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44018</th>\n",
              "      <td>44018</td>\n",
              "      <td>jawan killed in fire at army depot in maha</td>\n",
              "      <td>as many a  defence security corps  dsc  jawan ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16117</th>\n",
              "      <td>16117</td>\n",
              "      <td>hyperloop to reduce punemumbai travel time to ...</td>\n",
              "      <td>usbased highspeed transportation systthem hype...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8626</th>\n",
              "      <td>8626</td>\n",
              "      <td>universe is expanding up to   faster than expe...</td>\n",
              "      <td>a new study using nasa     s hubble space tele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30235</th>\n",
              "      <td>30235</td>\n",
              "      <td>indiaborn scientist win crore mit award</td>\n",
              "      <td>indiaborn scientist ramesh raskar wa awarded  ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                              Short\n",
              "19972       19972  ...  virat kohli average  in five inning at the apc...\n",
              "44018       44018  ...  as many a  defence security corps  dsc  jawan ...\n",
              "16117       16117  ...  usbased highspeed transportation systthem hype...\n",
              "8626         8626  ...  a new study using nasa     s hubble space tele...\n",
              "30235       30235  ...  indiaborn scientist ramesh raskar wa awarded  ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPcXPs9ZEJCz"
      },
      "source": [
        "data.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True) #떨궈서 필요한 거만 남기기, inplay=True -> 원본에 그대로 적용\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NkyyFAiELA3"
      },
      "source": [
        "data.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LALBcL_EQ1B"
      },
      "source": [
        "article = data['Short']\n",
        "summary = data['Headline']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux7nI0s8Evsk"
      },
      "source": [
        "article[25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0gyEi6kE2Ug"
      },
      "source": [
        " summary[25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUqvCgpoE691"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmOs30_cE8au"
      },
      "source": [
        "# 텍스트 전처리 \n",
        "## 여러 조건을 바꿔가며 실험해 보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xXoJLZyNfV6"
      },
      "source": [
        "!pip install -q contractions==0.0.48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSlBvH4FOQDe"
      },
      "source": [
        "from contractions import contractions_dict # 아포트로스피등을 사용한 축약문을 바꿀 수 있도록 정리한 단어사전\n",
        "\n",
        "for key, value in list(contractions_dict.items())[:15]: # contraction_dict에서 15개 예시를 출력해보자.\n",
        "    print(f'{key} == {value}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjP-RrwNOT0y"
      },
      "source": [
        "def expand_contractions(text, contraction_map=contractions_dict):\n",
        "    # 정규표현식을 사용해서 축약된 단어들을 모두 찾아와서 바꾸기\n",
        "    contractions_keys = '|'.join(contraction_map.keys()) # contractio_map의 key에 '|'를 추가\n",
        "    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL) # contraction_pattern에 정규표현식 형태를 저장 ; 컴파일 옵션으로 . 메타 문자가 줄바꿈 문자(\\n)를 포함하여 모든 문자와 매치되도록함\n",
        "\n",
        "    def expand_match(contraction):# 축약문에 맞는 문자열을 찾는 함수\n",
        "        # 조건에 맞는 전체 하위 문자열을 가져오기\n",
        "        match = contraction.group(0)\n",
        "        expanded_contraction = contraction_map.get(match)\n",
        "        if not expand_contractions:\n",
        "            print(match)\n",
        "            return match\n",
        "        return expanded_contraction\n",
        "\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text) # 축약문을 해당하는 문자열로 변환\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text \n",
        "\n",
        "# 함수가 잘 작동하는지 테스트 해보자\n",
        "expand_contractions(\"we'll gonna to expand contractions i'd think\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js8CcbTbOhaW"
      },
      "source": [
        "# 앞에서 만들어 두었던 expand_contractions 함수를 적용\n",
        "article = article.apply(expand_contractions)\n",
        "summary = summary.apply(expand_contractions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoRnZBnaPDIm"
      },
      "source": [
        "#테스트 해보자\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gP4DWzbPVkr"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords') #불용어 사전 다운로드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrj43fkPcyz"
      },
      "source": [
        "# nltk라이브러리에서 불용어 리스트를 불러워서 텍스트내의 불용어들을 제거하는 함수 작성\n",
        "def delete_stopwords(text):\n",
        "    _stopwords = stopwords.words('english') \n",
        "    _stopwords.append('cnn') # 불용어 사전에 (cnn)을 추가함. 이는 문서의 의미에 영향을 끼치지 않을 것이기 때문. 괄호지우는 작업도 정규표현식으로 더했더니 cnn만 남아서 그냥 cnn은 다 지워 버리는걸로 결정함\n",
        "    text = text.split()\n",
        "    word_list = [word for word in text if word not in _stopwords]\n",
        "    return ' '.join(word_list)\n",
        "\n",
        "# 함수를 테스트 해보자\n",
        "remove_stopwords_from_text(\"Love means never having to say you're sorry\") # 의미가 있는 단어 having이 통채로 사라져버렸다!! 적용하지 않는게 좋을 듯 하다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sI-YckxQaQa"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tknPfy1QNdl"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')  #lemmatize를 위한 단어사전"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTcr7fwdQbor"
      },
      "source": [
        "# 표제어추출을 위한 함수 작성\n",
        "def lemmatize_text(data):\n",
        "    data = nltk.word_tokenize(data)\n",
        "    lemtzr=WordNetLemmatizer()\n",
        "    out_data=\" \"\n",
        "    for words in data:\n",
        "        out_data+= lemtzr.lemmatize(words)+\" \"\n",
        "    return out_data\n",
        "\n",
        "#함수 테스트 해보자\n",
        "lemmatize_text(\"she goes to the hospitals\") # 동사에 붙는 s, 복수에 붙는 s 등이 사라지는 것을 확인할 수 있다! # 이를 통해 단어 사전을 줄임으로서 문장의 의미를 최대한 유지하면서 연산량을 줄일 수 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUuT49BAQr-Q"
      },
      "source": [
        "#표제어 추출 실행\n",
        "article=article.apply(lemmatize_text) # 원문에는 적용하지 않는 방법도 시도해 보자 # 아! 서비스에는 raw-senctence가 들어오니까 적용하면 안된다!!, 혹은 평가단계에서도 적용하여야 한다.\n",
        "summary=summary.apply(lemmatize_text) # summary에만 적용하자"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-e_wwKCRtSl"
      },
      "source": [
        "article[25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8X-GWyNSDCW"
      },
      "source": [
        "summary[25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx523kIQSVUf"
      },
      "source": [
        "#영어는 길이가 짧은 단어는 의미가 없다고 함\n",
        "#그런데 us와 같은 단어는 길이가 짧지만 경우에 따라서 큰 의미를 지니고 있을수도 있다고 생각이 들었음.\n",
        "#해서 길이가 짧은 단어를 지울지 말지는 고민해보고, 성능비교를 할때 적용해서 비교해보든지 해야겠다고 생각하였음.\n",
        "#길이가 짧은 단어를 제거하려면 다음과 같이 하면된다.\n",
        "#data.highlights=data.highlights.apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "#data.article=data.article.apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JOxNQRFFK6K"
      },
      "source": [
        "# 디코더의 작동을 위해 문장의 시작과 끝을 표시\n",
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tg8VT-AFUbu"
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' #정규표현식으로 필터 정의\n",
        "oov_token = '<unk>' # 단어장에 출현하지 않은, 모르는 단어를 표시할 토큰"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZKqwvf9Fjbn"
      },
      "source": [
        "# 케라스로 텍스트 토크나이징하는 토크나이징 메소드 생성\n",
        "article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token) # 필터 적용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEi8DQ63FsSm"
      },
      "source": [
        "# 데이터에 메소드 적용\n",
        "article_tokenizer.fit_on_texts(article)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNx9FyMEF-HQ"
      },
      "source": [
        "# 정수 시퀀스로 변경하고 입력시퀀스와 타깃 시퀀스로 지정\n",
        "inputs = article_tokenizer.texts_to_sequences(article)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnDUXoO9GKP2"
      },
      "source": [
        "# 확인해보자\n",
        "summary_tokenizer.texts_to_sequences([\"I'm a bad guy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKPTjP2jGWhS"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[1, 10, 1227, 4955]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XjRYwJhTUkS"
      },
      "source": [
        "#그렇다면 contractions를 적용한 문장은?\n",
        "summary_tokenizer.texts_to_sequences([\"I would like to go home\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8HQFMyWTZSG"
      },
      "source": [
        "# unk 토큰없이 아주 잘된다.\n",
        "summary_tokenizer.sequences_to_texts([[54, 467, 257, 4, 149, 220]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwS7O8sEGfRG"
      },
      "source": [
        "# 단어사전의 크기를 각각 지정\n",
        "encoder_vocab_size = len(article_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# 단어사전의 크기 확인\n",
        "encoder_vocab_size, decoder_vocab_size # 전처리를 까먹으면 단어사전 크기가 (76362, 29661) 가 나온다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVkShQ6LGlw-"
      },
      "source": [
        "article_lengths = pd.Series([len(x) for x in article])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J7snCo-Gzyf"
      },
      "source": [
        "article_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00CRXKBwG32r"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ6EeUgdG5g-"
      },
      "source": [
        "# 정수 시퀀스의 최대 길이를 지정\n",
        "encoder_maxlen = 420\n",
        "decoder_maxlen = 80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBV-A8RsmB_8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA6YQh7WmVii"
      },
      "source": [
        "inputs, inputs_val, targets, targets_val = train_test_split(inputs, targets, test_size=0.2, random_state=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2_ESgDgHAXl"
      },
      "source": [
        "# padding과 truncating  ; 이걸 안해서 문제가 생겼던 건 아닐까\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post') \n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
        "inputs_val=tf.keras.preprocessing.sequence.pad_sequences(inputs_val,maxlen=encoder_maxlen,padding='post',truncating='post')\n",
        "targets_val=tf.keras.preprocessing.sequence.pad_sequences(targets_val,maxlen=decoder_maxlen,padding=\"post\",truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EPZJMFKHaZk"
      },
      "source": [
        "# 텐서를 int32 타입으로 캐스팅\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)\n",
        "inputs_val=tf.cast(inputs_val,dtype=tf.int32)\n",
        "targets_val=tf.cast(targets_val,dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C72yGbptmKoQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvg01ibCHgzg"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "#텐서를 사용하여 데이터셋을 초기화 ; \n",
        "\n",
        "#The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, \n",
        "#removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.\n",
        "#This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements.\n",
        "#For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
        "#For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer.\n",
        "#Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_val=tf.data.Dataset.from_tensor_slices((inputs_val,targets_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE*2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8d8YQSUHlyk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaXciuLeHmWj"
      },
      "source": [
        "# 단어에 위치 개념을 추가하기 위한 함수 정의 ;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYYzQoyaJdSf"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNS7l-xUJd-G"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # 짝수 인덱스는 sin\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # 홀수 인덱스는 cos\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT0IdZP7J2l3"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYm3SRKdKASf"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_jxbFNuKBAs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuiqvR1NKCsb"
      },
      "source": [
        "# Attn 함수들 ; Attention is all you need github에서 가져옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtqKcv3KDDe"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True) # transposed key와 행렬곱\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # normalize를 위해 차원수로 나눠주기\n",
        "\n",
        "    if mask is not None: # 마스킹할 영역 준비, mask가 있으면\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) #소프트맥스에 통과시켜 확률값으로 변환\n",
        "\n",
        "    output = tf.matmul(attention_weights, v) # value와 행렬곱\n",
        "    return output, attention_weights # 컨텍스트 벡터를 리턴"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JanUJAKHva"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # 몇개로 나눠줄 것인가\n",
        "        self.d_model = d_model # 임베딩 차원수\n",
        "\n",
        "        assert d_model % self.num_heads == 0 \n",
        "\n",
        "        self.depth = d_model // self.num_heads #헤드 갯수로 나주어준다\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model) \n",
        "        \n",
        "    def split_heads(self, x, batch_size): # 헤드 나누기\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)  # 각각 linear dense에 통과시킨후\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size) #배치사이즈에 따라 나눈다.\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask) #스케일드 닷 어텐션\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # 원래 모양으로 돌려주기 위해\n",
        "        output = self.dense(concat_attention) # concat\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ3VKD6OKTZB"
      },
      "source": [
        "#피드포워드 네트워크\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([ #레이어 순차적으로\n",
        "        tf.keras.layers.Dense(dff, activation='relu'), # 뉴럴 수, relu\n",
        "        tf.keras.layers.Dense(d_model) # 임베딩 차원\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiZjOlX2KXR4"
      },
      "source": [
        "#기초적인 인코더레이어 유닛\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads) # 멀티헤드 어텐션\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff) # 피드포워드\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # 노멀라이즈\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6) #노멀라이즈\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate) #드롭아웃\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask) \n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO-pLNcFKj9r"
      },
      "source": [
        "#기초적인 디코더레이어 유닛\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRVW5igkKncn"
      },
      "source": [
        "#아까 정의한 인코더 레이어를 여러장 겹쳐서 인코더를 정의\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1): # d_model 워드 임베딩의 차원 ,num_layer: 적층할 레이어 갯수, dff: 피드포워드 네트워크의 뉴런갯수\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) # 워드 임베딩을 전달\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model) #포지션 인코딩 \n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] #num_layer는 4장, 4장의 인코더 레이어로 구성\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate) # dropout 설정\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x) \n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMYe0r1KK1L2"
      },
      "source": [
        "#마찬가지로 디코더 레이어를 여러장 겹쳐서 디코더 정의\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model) # 임베딩 레이어\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model) #포지션 인코딩\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] \n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBs3arEcK6FW"
      },
      "source": [
        "# 인코더 디코더를 합쳐서 트랜스포머 정의\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate) #인코더\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate) # 디코더\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size) #Dens 레이어로 마무리\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask) \n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwuYoeyiK-tI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6yq8sQ8LAIu"
      },
      "source": [
        "# 모델 훈련 단계\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBra1bszLDB9"
      },
      "source": [
        "# 하이퍼 파라미터 조정\n",
        "num_layers = 4 #\n",
        "d_model = 128 # embedding dim\n",
        "dff = 512 #피드 포워드에 집어넣을 뉴런 수\n",
        "num_heads = 8 #\n",
        "EPOCHS = 37 #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RjSxihwLLHo"
      },
      "source": [
        "#커스텀 러닝 스케쥴 사용\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYZh-Q-QLT0m"
      },
      "source": [
        "# 러닝 레이트와 옵티마이저 설정\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEo3TTdPLaJP"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpgTAa49LB4c"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrFNQf3yLgWs"
      },
      "source": [
        "# loss metric 설정\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syVId44OLkxy"
      },
      "source": [
        "transformer = Transformer(num_layers,d_model,num_heads,dff,encoder_vocab_size,decoder_vocab_size,pe_input=encoder_vocab_size,pe_target=decoder_vocab_size,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v92B48VqLr_a"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L45_GN-YKBwy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JjbJJP8Lu5W"
      },
      "source": [
        "# 훈련 체크포인트 설정\n",
        "checkpoint_path = \"checkpoints_01\" # 폴더 경로\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer) \n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('체크포인트 복구됨')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShnPdQ_NL-bW"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esWuiMVXCWcy"
      },
      "source": [
        "history={'val':[],'train':[]}\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miBcmOG7lsdJ"
      },
      "source": [
        "def hist(history):\n",
        "    plt.title('Loss')\n",
        "\n",
        "    x= [i[0] for i in history['val']]\n",
        "    y=[i[1] for i in history['val']]\n",
        "    plt.plot(x,y,'x-')\n",
        "    \n",
        "    x= [i[0] for i in history['train']]\n",
        "    y=[i[1] for i in history['train']]    \n",
        "    plt.plot(x,y,'o-')\n",
        "\n",
        "    plt.legend(['validation','train'])\n",
        "    plt.show()\n",
        "    print('smallest val loss:', sorted(history['val'],key=lambda x: x[1])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDEecHUIl26t"
      },
      "source": [
        "EPOCHS = 37\n",
        "not_progressing = 0\n",
        "# Computes the (weighted) mean of the given loss values.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvWbB-p6pmWd"
      },
      "source": [
        "def validate():\n",
        "    print('validation started ...')\n",
        "    val_loss.reset_states()\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_val):    \n",
        "        tar_inp = tar[:, :-1] \n",
        "        tar_real = tar[:, 1:] \n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "        predictions, _ = transformer(inp, tar_inp, False,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        val_loss(loss)\n",
        "    print('\\n* Validation loss: {} '.format(val_loss.result()) )\n",
        "    return val_loss.result()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWI5ImLWMNsX"
      },
      "source": [
        "#훈련 단계\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        if batch % 429 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "            val_loss_ = validate().numpy()\n",
        "            history['val'].append((epoch,val_loss_))\n",
        "            print ('\\n* Train Loss {:.4f}'.format(train_loss.result()))\n",
        "            history['train'].append((epoch,train_loss.result().numpy())) \n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "    hist(history)\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKFTB_JnMZlA"
      },
      "source": [
        "# 평가단계\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViwgG6iCMaBk"
      },
      "source": [
        "# 디코더에서 한 번에 한 단어씩 예측하여 출력에 추가한 다음 \n",
        "# 전체 시퀀스를 디코더의 입력으로 가져와서 문장의 끝이 나타날 때까지 반복\n",
        "def evaluate(input_document):\n",
        "    input_document = article_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrAz2odfMf7R"
      },
      "source": [
        "# 입력을 요약하는 함수\n",
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F8laQg3Mk0y"
      },
      "source": [
        "# 테스트 해보자\n",
        "summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faoRBESWVGAn"
      },
      "source": [
        "# Rouge metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWUiWr6zVJV-"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSYVwL8iWBjl"
      },
      "source": [
        "from rouge import Rouge \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LzyfNlvWMRC"
      },
      "source": [
        "hypothesis = \"the transcript is a written version of each day 's cnn student news program use this transcript to help students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
        "#추측\n",
        "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
        "\n",
        "#원래 요약문\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hypothesis, reference)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yCTsU4Jv3CX"
      },
      "source": [
        "# 원문은 article[21], A police officer in Lucknow, Nagesh Mishra, was suspended from duty after he made everyone swear to a vow of cleanliness while he was himself consuming tobacco. Notably,\n",
        "#Uttar Pradesh CM Yogi Adityanath has directed officials against consuming pan masala in office premises.\n",
        "#Over 100 Uttar Pradesh cops have been suspended over discipline-related issues ever since Adityanath assumed office.\n",
        "\n",
        "hypothesis=summarize(\"A police officer in Lucknow, Nagesh Mishra, was suspended from duty after he made everyone swear to a vow of cleanliness while he was himself consuming tobacco. Notably,Uttar Pradesh CM Yogi Adityanath has directed officials against consuming pan masala in office premises.Over 100 Uttar Pradesh cops have been suspended over discipline-related issues ever since Adityanath assumed office.\")\n",
        "\n",
        "reference=\"UP cop suspended for consuming tobacco\"\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hypothesis, reference,avg=True)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1NCnveFxbO1"
      },
      "source": [
        "article[300], summary[300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4YamQd-yDz4"
      },
      "source": [
        "summarize(\"A police officer in Lucknow, Nagesh Mishra, was suspended from duty after he made everyone swear to a vow of cleanliness while he was himself consuming tobacco. Notably,Uttar Pradesh CM Yogi Adityanath has directed officials against consuming pan masala in office premises.Over 100 Uttar Pradesh cops have been suspended over discipline-related issues ever since Adityanath assumed office.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C14qXmQ_ZkPr"
      },
      "source": [
        "summary[40000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOpYRyoPwE1b"
      },
      "source": [
        "for i in range (len(article)):\n",
        "  _hypothesis=summarize(article[i])\n",
        "  _reference=summary[i]\n",
        "\n",
        "  rouge=Rouge()\n",
        "  scores=rouge.get_scores(_hypothesis,_reference,avg=True)\n",
        "  print(scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnWsMX1uWQxf"
      },
      "source": [
        "# 원문은 article[21], A police officer in Lucknow, Nagesh Mishra, was suspended from duty after he made everyone swear to a vow of cleanliness while he was himself consuming tobacco. Notably,\n",
        "#Uttar Pradesh CM Yogi Adityanath has directed officials against consuming pan masala in office premises.\n",
        "#Over 100 Uttar Pradesh cops have been suspended over discipline-related issues ever since Adityanath assumed office.\n",
        "\n",
        "hypothesis=summarize(\"A police officer in Lucknow, Nagesh Mishra, was suspended from duty after he made everyone swear to a vow of cleanliness while he was himself consuming tobacco. Notably,Uttar Pradesh CM Yogi Adityanath has directed officials against consuming pan masala in office premises.Over 100 Uttar Pradesh cops have been suspended over discipline-related issues ever since Adityanath assumed office.\")\n",
        "\n",
        "reference=\"UP cop suspended for consuming tobacco\"\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hypothesis, reference,avg=True)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_B3tTtgYWuW"
      },
      "source": [
        "for i in range (len(article)):\n",
        "  _hypothesis=_hypothesis.append(summarize(article[i]))\n",
        "  _reference=_reference.append(summary[i])\n",
        "\n",
        "  rouge=Rouge()\n",
        "  scores=rouge.get_scores(_hypothesis,_reference,avg=True)\n",
        "  print(scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGCb-0mdLT9M"
      },
      "source": [
        "# 시험\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLgHS72BLWo2"
      },
      "source": [
        "summarize(\"A victory for one of the hardliners is not expected to derail talks in Vienna between Iran and world powers that are aimed at reviving the accord, which saw Iran agree to limit its nuclear programme in return for sanctions relief.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o6eHCBvL5Cf"
      },
      "source": [
        "# ToDo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axk3IB3NL2tJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goBVJtdHL21Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye-Sj0IjL3PM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}